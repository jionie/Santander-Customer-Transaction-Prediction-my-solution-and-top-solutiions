{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:46<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling num_cols\n",
      "scaling var_0\n",
      "scaling var_1\n",
      "scaling var_2\n",
      "scaling var_3\n",
      "scaling var_4\n",
      "scaling var_5\n",
      "scaling var_6\n",
      "scaling var_7\n",
      "scaling var_8\n",
      "scaling var_9\n",
      "scaling var_10\n",
      "scaling var_11\n",
      "scaling var_12\n",
      "scaling var_13\n",
      "scaling var_14\n",
      "scaling var_15\n",
      "scaling var_16\n",
      "scaling var_17\n",
      "scaling var_18\n",
      "scaling var_19\n",
      "scaling var_20\n",
      "scaling var_21\n",
      "scaling var_22\n",
      "scaling var_23\n",
      "scaling var_24\n",
      "scaling var_25\n",
      "scaling var_26\n",
      "scaling var_27\n",
      "scaling var_28\n",
      "scaling var_29\n",
      "scaling var_30\n",
      "scaling var_31\n",
      "scaling var_32\n",
      "scaling var_33\n",
      "scaling var_34\n",
      "scaling var_35\n",
      "scaling var_36\n",
      "scaling var_37\n",
      "scaling var_38\n",
      "scaling var_39\n",
      "scaling var_40\n",
      "scaling var_41\n",
      "scaling var_42\n",
      "scaling var_43\n",
      "scaling var_44\n",
      "scaling var_45\n",
      "scaling var_46\n",
      "scaling var_47\n",
      "scaling var_48\n",
      "scaling var_49\n",
      "scaling var_50\n",
      "scaling var_51\n",
      "scaling var_52\n",
      "scaling var_53\n",
      "scaling var_54\n",
      "scaling var_55\n",
      "scaling var_56\n",
      "scaling var_57\n",
      "scaling var_58\n",
      "scaling var_59\n",
      "scaling var_60\n",
      "scaling var_61\n",
      "scaling var_62\n",
      "scaling var_63\n",
      "scaling var_64\n",
      "scaling var_65\n",
      "scaling var_66\n",
      "scaling var_67\n",
      "scaling var_68\n",
      "scaling var_69\n",
      "scaling var_70\n",
      "scaling var_71\n",
      "scaling var_72\n",
      "scaling var_73\n",
      "scaling var_74\n",
      "scaling var_75\n",
      "scaling var_76\n",
      "scaling var_77\n",
      "scaling var_78\n",
      "scaling var_79\n",
      "scaling var_80\n",
      "scaling var_81\n",
      "scaling var_82\n",
      "scaling var_83\n",
      "scaling var_84\n",
      "scaling var_85\n",
      "scaling var_86\n",
      "scaling var_87\n",
      "scaling var_88\n",
      "scaling var_89\n",
      "scaling var_90\n",
      "scaling var_91\n",
      "scaling var_92\n",
      "scaling var_93\n",
      "scaling var_94\n",
      "scaling var_95\n",
      "scaling var_96\n",
      "scaling var_97\n",
      "scaling var_98\n",
      "scaling var_99\n",
      "scaling var_100\n",
      "scaling var_101\n",
      "scaling var_102\n",
      "scaling var_103\n",
      "scaling var_104\n",
      "scaling var_105\n",
      "scaling var_106\n",
      "scaling var_107\n",
      "scaling var_108\n",
      "scaling var_109\n",
      "scaling var_110\n",
      "scaling var_111\n",
      "scaling var_112\n",
      "scaling var_113\n",
      "scaling var_114\n",
      "scaling var_115\n",
      "scaling var_116\n",
      "scaling var_117\n",
      "scaling var_118\n",
      "scaling var_119\n",
      "scaling var_120\n",
      "scaling var_121\n",
      "scaling var_122\n",
      "scaling var_123\n",
      "scaling var_124\n",
      "scaling var_125\n",
      "scaling var_126\n",
      "scaling var_127\n",
      "scaling var_128\n",
      "scaling var_129\n",
      "scaling var_130\n",
      "scaling var_131\n",
      "scaling var_132\n",
      "scaling var_133\n",
      "scaling var_134\n",
      "scaling var_135\n",
      "scaling var_136\n",
      "scaling var_137\n",
      "scaling var_138\n",
      "scaling var_139\n",
      "scaling var_140\n",
      "scaling var_141\n",
      "scaling var_142\n",
      "scaling var_143\n",
      "scaling var_144\n",
      "scaling var_145\n",
      "scaling var_146\n",
      "scaling var_147\n",
      "scaling var_148\n",
      "scaling var_149\n",
      "scaling var_150\n",
      "scaling var_151\n",
      "scaling var_152\n",
      "scaling var_153\n",
      "scaling var_154\n",
      "scaling var_155\n",
      "scaling var_156\n",
      "scaling var_157\n",
      "scaling var_158\n",
      "scaling var_159\n",
      "scaling var_160\n",
      "scaling var_161\n",
      "scaling var_162\n",
      "scaling var_163\n",
      "scaling var_164\n",
      "scaling var_165\n",
      "scaling var_166\n",
      "scaling var_167\n",
      "scaling var_168\n",
      "scaling var_169\n",
      "scaling var_170\n",
      "scaling var_171\n",
      "scaling var_172\n",
      "scaling var_173\n",
      "scaling var_174\n",
      "scaling var_175\n",
      "scaling var_176\n",
      "scaling var_177\n",
      "scaling var_178\n",
      "scaling var_179\n",
      "scaling var_180\n",
      "scaling var_181\n",
      "scaling var_182\n",
      "scaling var_183\n",
      "scaling var_184\n",
      "scaling var_185\n",
      "scaling var_186\n",
      "scaling var_187\n",
      "scaling var_188\n",
      "scaling var_189\n",
      "scaling var_190\n",
      "scaling var_191\n",
      "scaling var_192\n",
      "scaling var_193\n",
      "scaling var_194\n",
      "scaling var_195\n",
      "scaling var_196\n",
      "scaling var_197\n",
      "scaling var_198\n",
      "scaling var_199\n",
      "scaling var_0_counts\n",
      "scaling var_1_counts\n",
      "scaling var_2_counts\n",
      "scaling var_3_counts\n",
      "scaling var_4_counts\n",
      "scaling var_5_counts\n",
      "scaling var_6_counts\n",
      "scaling var_7_counts\n",
      "scaling var_8_counts\n",
      "scaling var_9_counts\n",
      "scaling var_10_counts\n",
      "scaling var_11_counts\n",
      "scaling var_12_counts\n",
      "scaling var_13_counts\n",
      "scaling var_14_counts\n",
      "scaling var_15_counts\n",
      "scaling var_16_counts\n",
      "scaling var_17_counts\n",
      "scaling var_18_counts\n",
      "scaling var_19_counts\n",
      "scaling var_20_counts\n",
      "scaling var_21_counts\n",
      "scaling var_22_counts\n",
      "scaling var_23_counts\n",
      "scaling var_24_counts\n",
      "scaling var_25_counts\n",
      "scaling var_26_counts\n",
      "scaling var_27_counts\n",
      "scaling var_28_counts\n",
      "scaling var_29_counts\n",
      "scaling var_30_counts\n",
      "scaling var_31_counts\n",
      "scaling var_32_counts\n",
      "scaling var_33_counts\n",
      "scaling var_34_counts\n",
      "scaling var_35_counts\n",
      "scaling var_36_counts\n",
      "scaling var_37_counts\n",
      "scaling var_38_counts\n",
      "scaling var_39_counts\n",
      "scaling var_40_counts\n",
      "scaling var_41_counts\n",
      "scaling var_42_counts\n",
      "scaling var_43_counts\n",
      "scaling var_44_counts\n",
      "scaling var_45_counts\n",
      "scaling var_46_counts\n",
      "scaling var_47_counts\n",
      "scaling var_48_counts\n",
      "scaling var_49_counts\n",
      "scaling var_50_counts\n",
      "scaling var_51_counts\n",
      "scaling var_52_counts\n",
      "scaling var_53_counts\n",
      "scaling var_54_counts\n",
      "scaling var_55_counts\n",
      "scaling var_56_counts\n",
      "scaling var_57_counts\n",
      "scaling var_58_counts\n",
      "scaling var_59_counts\n",
      "scaling var_60_counts\n",
      "scaling var_61_counts\n",
      "scaling var_62_counts\n",
      "scaling var_63_counts\n",
      "scaling var_64_counts\n",
      "scaling var_65_counts\n",
      "scaling var_66_counts\n",
      "scaling var_67_counts\n",
      "scaling var_68_counts\n",
      "scaling var_69_counts\n",
      "scaling var_70_counts\n",
      "scaling var_71_counts\n",
      "scaling var_72_counts\n",
      "scaling var_73_counts\n",
      "scaling var_74_counts\n",
      "scaling var_75_counts\n",
      "scaling var_76_counts\n",
      "scaling var_77_counts\n",
      "scaling var_78_counts\n",
      "scaling var_79_counts\n",
      "scaling var_80_counts\n",
      "scaling var_81_counts\n",
      "scaling var_82_counts\n",
      "scaling var_83_counts\n",
      "scaling var_84_counts\n",
      "scaling var_85_counts\n",
      "scaling var_86_counts\n",
      "scaling var_87_counts\n",
      "scaling var_88_counts\n",
      "scaling var_89_counts\n",
      "scaling var_90_counts\n",
      "scaling var_91_counts\n",
      "scaling var_92_counts\n",
      "scaling var_93_counts\n",
      "scaling var_94_counts\n",
      "scaling var_95_counts\n",
      "scaling var_96_counts\n",
      "scaling var_97_counts\n",
      "scaling var_98_counts\n",
      "scaling var_99_counts\n",
      "scaling var_100_counts\n",
      "scaling var_101_counts\n",
      "scaling var_102_counts\n",
      "scaling var_103_counts\n",
      "scaling var_104_counts\n",
      "scaling var_105_counts\n",
      "scaling var_106_counts\n",
      "scaling var_107_counts\n",
      "scaling var_108_counts\n",
      "scaling var_109_counts\n",
      "scaling var_110_counts\n",
      "scaling var_111_counts\n",
      "scaling var_112_counts\n",
      "scaling var_113_counts\n",
      "scaling var_114_counts\n",
      "scaling var_115_counts\n",
      "scaling var_116_counts\n",
      "scaling var_117_counts\n",
      "scaling var_118_counts\n",
      "scaling var_119_counts\n",
      "scaling var_120_counts\n",
      "scaling var_121_counts\n",
      "scaling var_122_counts\n",
      "scaling var_123_counts\n",
      "scaling var_124_counts\n",
      "scaling var_125_counts\n",
      "scaling var_126_counts\n",
      "scaling var_127_counts\n",
      "scaling var_128_counts\n",
      "scaling var_129_counts\n",
      "scaling var_130_counts\n",
      "scaling var_131_counts\n",
      "scaling var_132_counts\n",
      "scaling var_133_counts\n",
      "scaling var_134_counts\n",
      "scaling var_135_counts\n",
      "scaling var_136_counts\n",
      "scaling var_137_counts\n",
      "scaling var_138_counts\n",
      "scaling var_139_counts\n",
      "scaling var_140_counts\n",
      "scaling var_141_counts\n",
      "scaling var_142_counts\n",
      "scaling var_143_counts\n",
      "scaling var_144_counts\n",
      "scaling var_145_counts\n",
      "scaling var_146_counts\n",
      "scaling var_147_counts\n",
      "scaling var_148_counts\n",
      "scaling var_149_counts\n",
      "scaling var_150_counts\n",
      "scaling var_151_counts\n",
      "scaling var_152_counts\n",
      "scaling var_153_counts\n",
      "scaling var_154_counts\n",
      "scaling var_155_counts\n",
      "scaling var_156_counts\n",
      "scaling var_157_counts\n",
      "scaling var_158_counts\n",
      "scaling var_159_counts\n",
      "scaling var_160_counts\n",
      "scaling var_161_counts\n",
      "scaling var_162_counts\n",
      "scaling var_163_counts\n",
      "scaling var_164_counts\n",
      "scaling var_165_counts\n",
      "scaling var_166_counts\n",
      "scaling var_167_counts\n",
      "scaling var_168_counts\n",
      "scaling var_169_counts\n",
      "scaling var_170_counts\n",
      "scaling var_171_counts\n",
      "scaling var_172_counts\n",
      "scaling var_173_counts\n",
      "scaling var_174_counts\n",
      "scaling var_175_counts\n",
      "scaling var_176_counts\n",
      "scaling var_177_counts\n",
      "scaling var_178_counts\n",
      "scaling var_179_counts\n",
      "scaling var_180_counts\n",
      "scaling var_181_counts\n",
      "scaling var_182_counts\n",
      "scaling var_183_counts\n",
      "scaling var_184_counts\n",
      "scaling var_185_counts\n",
      "scaling var_186_counts\n",
      "scaling var_187_counts\n",
      "scaling var_188_counts\n",
      "scaling var_189_counts\n",
      "scaling var_190_counts\n",
      "scaling var_191_counts\n",
      "scaling var_192_counts\n",
      "scaling var_193_counts\n",
      "scaling var_194_counts\n",
      "scaling var_195_counts\n",
      "scaling var_196_counts\n",
      "scaling var_197_counts\n",
      "scaling var_198_counts\n",
      "scaling var_199_counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200, 2)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200, 64)           192       \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 200, 64)           12800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200, 64)           256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200, 8)            520       \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 200, 8)            1600      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1601      \n",
      "=================================================================\n",
      "Total params: 16,969\n",
      "Trainable params: 16,841\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 336076 samples, validate on 40001 samples\n",
      "Epoch 1/20\n",
      "336076/336076 [==============================] - 13s 38us/step - loss: 0.2652 - val_loss: 0.1913\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19135, saving model to nn0.hdf5\n",
      "Epoch 2/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2338 - val_loss: 0.1855\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19135 to 0.18547, saving model to nn0.hdf5\n",
      "Epoch 3/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2282 - val_loss: 0.1882\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.18547\n",
      "Epoch 4/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2253 - val_loss: 0.1844\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.18547 to 0.18436, saving model to nn0.hdf5\n",
      "Epoch 5/20\n",
      "336076/336076 [==============================] - 9s 26us/step - loss: 0.2234 - val_loss: 0.1912\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.18436\n",
      "Epoch 6/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2230 - val_loss: 0.1814\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.18436 to 0.18136, saving model to nn0.hdf5\n",
      "Epoch 7/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2220 - val_loss: 0.1916\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.18136\n",
      "Epoch 8/20\n",
      "336076/336076 [==============================] - 9s 28us/step - loss: 0.2203 - val_loss: 0.2103\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.18136\n",
      "Epoch 9/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2199 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.18136 to 0.18132, saving model to nn0.hdf5\n",
      "Epoch 10/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2194 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.18132\n",
      "Epoch 11/20\n",
      "336076/336076 [==============================] - 9s 26us/step - loss: 0.2187 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.18132\n",
      "Epoch 12/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2159 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.18132 to 0.18023, saving model to nn0.hdf5\n",
      "Epoch 13/20\n",
      "336076/336076 [==============================] - 9s 28us/step - loss: 0.2153 - val_loss: 0.1976\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.18023\n",
      "Epoch 14/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2153 - val_loss: 0.1851\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.18023\n",
      "Epoch 15/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2148 - val_loss: 0.1898\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.18023\n",
      "Epoch 16/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2146 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.18023 to 0.17907, saving model to nn0.hdf5\n",
      "Epoch 17/20\n",
      "336076/336076 [==============================] - 10s 29us/step - loss: 0.2144 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.17907\n",
      "Epoch 18/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2143 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.17907\n",
      "Epoch 19/20\n",
      "336076/336076 [==============================] - 10s 29us/step - loss: 0.2142 - val_loss: 0.1957\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.17907\n",
      "Epoch 20/20\n",
      "336076/336076 [==============================] - 9s 28us/step - loss: 0.2140 - val_loss: 0.1882\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.17907\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200, 2)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200, 64)           192       \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 200, 64)           12800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 200, 64)           256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200, 8)            520       \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 200, 8)            1600      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 1601      \n",
      "=================================================================\n",
      "Total params: 16,969\n",
      "Trainable params: 16,841\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 336076 samples, validate on 40001 samples\n",
      "Epoch 1/20\n",
      "336076/336076 [==============================] - 10s 29us/step - loss: 0.2736 - val_loss: 0.1969\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19692, saving model to nn1.hdf5\n",
      "Epoch 2/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2307 - val_loss: 0.1867\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19692 to 0.18672, saving model to nn1.hdf5\n",
      "Epoch 3/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2257 - val_loss: 0.1923\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.18672\n",
      "Epoch 4/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2226 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.18672 to 0.18358, saving model to nn1.hdf5\n",
      "Epoch 5/20\n",
      "336076/336076 [==============================] - 9s 28us/step - loss: 0.2213 - val_loss: 0.2022\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.18358\n",
      "Epoch 6/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2199 - val_loss: 0.1879\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.18358\n",
      "Epoch 7/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2188 - val_loss: 0.1833\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.18358 to 0.18333, saving model to nn1.hdf5\n",
      "Epoch 8/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2188 - val_loss: 0.1915\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.18333\n",
      "Epoch 9/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2178 - val_loss: 0.1925\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.18333\n",
      "Epoch 10/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2175 - val_loss: 0.1929\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.18333\n",
      "Epoch 11/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2167 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.18333 to 0.18301, saving model to nn1.hdf5\n",
      "Epoch 12/20\n",
      "336076/336076 [==============================] - 9s 27us/step - loss: 0.2166 - val_loss: 0.1835\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.18301\n",
      "Epoch 13/20\n",
      "336076/336076 [==============================] - 9s 28us/step - loss: 0.2162 - val_loss: 0.1888\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.18301\n",
      "Epoch 14/20\n",
      "193792/336076 [================>.............] - ETA: 3s - loss: 0.2153"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MODEL_PATH = ''\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv',index_col='ID_code')\n",
    "test_df = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv',index_col='ID_code')\n",
    "\n",
    "synthetic_indices = np.load('../input/synthetissantandersamples/synthetic_samples_indexes.npy')\n",
    "mask=np.full(len(test_df),True,dtype=bool)\n",
    "mask[synthetic_indices]=False\n",
    "test_df_nonsynthetic = test_df.iloc[mask].reset_index(drop=True).copy()\n",
    "\n",
    "\n",
    "y = train_df.pop('target')\n",
    "target = y\n",
    "\n",
    "tr_te = pd.concat([train_df,test_df])\n",
    "\n",
    "num_cols = [c for c in train_df.columns]\n",
    "\n",
    "for f in tqdm(num_cols):\n",
    "    tr_te[f+'_counts'] = tr_te[f].map(pd.concat([train_df[f], test_df_nonsynthetic[f]], axis=0).value_counts().to_dict(), na_action='ignore')\n",
    "    tr_te[f+'_counts'] = tr_te[f+'_counts'].fillna(1)\n",
    "\n",
    "\n",
    "count_cols = [f+'_counts' for f in num_cols]\n",
    "\n",
    "\n",
    "from scipy.special import erfinv\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def rankgauss(x):\n",
    "    r = (rankdata(x) - 1) / len(x)  # to [0,1]\n",
    "    r = 2 * r - 1  # to [-1,1]\n",
    "    r = np.clip(r, -0.99, 0.99)\n",
    "    r2 = erfinv(r)\n",
    "    return r2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('scaling num_cols')\n",
    "for col in num_cols + count_cols:\n",
    "    print('scaling {}'.format(col))\n",
    "    col_mean = tr_te[col].mean()\n",
    "    col_std = tr_te[col].std()\n",
    "    tr_te[col].fillna(col_mean, inplace=True)\n",
    "    tr_te[col] = rankgauss(tr_te[col].values)\n",
    "\n",
    "\n",
    "train_df = tr_te[0:train_df.shape[0]]\n",
    "test_df = tr_te[train_df.shape[0]:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = np.stack([train_df[num_cols].values,train_df[count_cols].values],axis = -1)\n",
    "X_test = np.stack([test_df[num_cols].values,test_df[count_cols].values],axis = -1)\n",
    "#X = train_df[num_cols].values\n",
    "\n",
    "def augment_counts(x, y, t_pos, t_neg):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t_pos):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(200):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "            #x1[:,c+200] = x1[ids][:,c+200]\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t_neg):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(200):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "            #x1[:,c+200] = x1[ids][:,c+200]\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn])\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    return x,y\n",
    "\n",
    "from keras import layers as L\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "def build_model():\n",
    "    inp = L.Input((200,2))\n",
    "    x = L.Dense(64)(inp)\n",
    "    x = L.PReLU()(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.2)(x)\n",
    "    x = L.Dense(8)(x)\n",
    "    x = L.PReLU()(x)\n",
    "    x = L.Flatten()(x)\n",
    "    out = L.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    m = Model(inp,out)\n",
    "    print(m.summary())\n",
    "    return m\n",
    "\n",
    "num_folds = 5\n",
    "folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "splits = list(folds.split(train_df.values, target.values))\n",
    "\n",
    "oof_preds = np.zeros(y.shape)\n",
    "test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "for fold_ in [0, 1, 2, 3, 4]:\n",
    "    trn_idx, val_idx = splits[fold_]\n",
    "\n",
    "    X_train, y_train = X[trn_idx], y[trn_idx]\n",
    "    X_valid, y_valid = X[val_idx], y[val_idx]\n",
    "    \n",
    "    X_train, y_train = augment_counts(X_train, y_train, 2, 1)\n",
    "\n",
    "    m = build_model()\n",
    "    ckpt = ModelCheckpoint(MODEL_PATH + 'nn{}.hdf5'.format(fold_), save_best_only=True, verbose=True)\n",
    "    pl = ReduceLROnPlateau(factor=0.5,patience=5)\n",
    "    m.compile(optimizer=Adam(), loss=binary_crossentropy)\n",
    "    m.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, verbose=1, callbacks=[ckpt,pl], batch_size = 256)\n",
    "    m.load_weights(MODEL_PATH + 'nn{}.hdf5'.format(fold_))\n",
    "    oof_preds[val_idx] = m.predict(X_valid)[:, 0]\n",
    "    test_preds += m.predict(X_test)[:,0]\n",
    "test_preds/= 5\n",
    "\n",
    "\n",
    "\n",
    "np.save(MODEL_PATH + 'oof_NN13b_aug.npy',oof_preds)\n",
    "np.save(MODEL_PATH + 'sub_NN13b_aug.npy',test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/santander-customer-transaction-prediction/sample_submission.csv')\n",
    "submission['target'] = test_preds\n",
    "submission.to_csv(MODEL_PATH + 'submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
